{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d69394d",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721683b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "import matplotlib.pyplot as plt\n",
    "from yolov8.utils import xywh2xyxy, draw_detections, multiclass_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9815dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv8:\n",
    "    def __init__(self, path, conf_thres=0.7, iou_thres=0.5):\n",
    "        self.conf_threshold = conf_thres\n",
    "        self.iou_threshold = iou_thres\n",
    "\n",
    "        # Initialize model\n",
    "        self.initialize_model(path)\n",
    "\n",
    "    def __call__(self, image):\n",
    "        return self.detect_objects(image)\n",
    "\n",
    "    def initialize_model(self, path):\n",
    "        self.session = onnxruntime.InferenceSession(path,\n",
    "                                                    providers=onnxruntime.get_available_providers())\n",
    "        # Get model info\n",
    "        self.get_input_details()\n",
    "        self.get_output_details()\n",
    "\n",
    "\n",
    "    def detect_objects(self, image):\n",
    "        input_tensor = self.prepare_input(image)\n",
    "\n",
    "        # Perform inference on the image\n",
    "        outputs = self.inference(input_tensor)\n",
    "\n",
    "        self.boxes, self.scores, self.class_ids = self.process_output(outputs)\n",
    "\n",
    "        return self.boxes, self.scores, self.class_ids\n",
    "\n",
    "    def prepare_input(self, image):\n",
    "        self.img_height, self.img_width = image.shape[:2]\n",
    "\n",
    "        input_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Resize input image\n",
    "        input_img = cv2.resize(input_img, (self.input_width, self.input_height))\n",
    "\n",
    "        # Scale input pixel values to 0 to 1\n",
    "        input_img = input_img / 255.0\n",
    "        input_img = input_img.transpose(2, 0, 1)\n",
    "        input_tensor = input_img[np.newaxis, :, :, :].astype(np.float32)\n",
    "\n",
    "        return input_tensor\n",
    "\n",
    "\n",
    "    def inference(self, input_tensor):\n",
    "        start = time.perf_counter()\n",
    "        outputs = self.session.run(self.output_names, {self.input_names[0]: input_tensor})\n",
    "\n",
    "        # print(f\"Inference time: {(time.perf_counter() - start)*1000:.2f} ms\")\n",
    "        return outputs\n",
    "\n",
    "    def process_output(self, output):\n",
    "        predictions = np.squeeze(output[0]).T\n",
    "\n",
    "        # Filter out object confidence scores below threshold\n",
    "        scores = np.max(predictions[:, 4:], axis=1)\n",
    "        predictions = predictions[scores > self.conf_threshold, :]\n",
    "        scores = scores[scores > self.conf_threshold]\n",
    "\n",
    "        if len(scores) == 0:\n",
    "            return [], [], []\n",
    "\n",
    "        # Get the class with the highest confidence\n",
    "        class_ids = np.argmax(predictions[:, 4:], axis=1)\n",
    "\n",
    "        # Get bounding boxes for each object\n",
    "        boxes = self.extract_boxes(predictions)\n",
    "\n",
    "        # Apply non-maxima suppression to suppress weak, overlapping bounding boxes\n",
    "        # indices = nms(boxes, scores, self.iou_threshold)\n",
    "        indices = multiclass_nms(boxes, scores, class_ids, self.iou_threshold)\n",
    "\n",
    "        return boxes[indices], scores[indices], class_ids[indices]\n",
    "\n",
    "    def extract_boxes(self, predictions):\n",
    "        # Extract boxes from predictions\n",
    "        boxes = predictions[:, :4]\n",
    "\n",
    "        # Scale boxes to original image dimensions\n",
    "        boxes = self.rescale_boxes(boxes)\n",
    "\n",
    "        # Convert boxes to xyxy format\n",
    "        boxes = xywh2xyxy(boxes)\n",
    "\n",
    "        return boxes\n",
    "\n",
    "    def rescale_boxes(self, boxes):\n",
    "\n",
    "        # Rescale boxes to original image dimensions\n",
    "        input_shape = np.array([self.input_width, self.input_height, self.input_width, self.input_height])\n",
    "        boxes = np.divide(boxes, input_shape, dtype=np.float32)\n",
    "        boxes *= np.array([self.img_width, self.img_height, self.img_width, self.img_height])\n",
    "        return boxes\n",
    "\n",
    "    def draw_detections(self, image, draw_scores=True, mask_alpha=0.4):\n",
    "\n",
    "        return draw_detections(image, self.boxes, self.scores,self.class_ids, mask_alpha)\n",
    "\n",
    "    def get_input_details(self):\n",
    "        model_inputs = self.session.get_inputs()\n",
    "        self.input_names = [model_inputs[i].name for i in range(len(model_inputs))]\n",
    "\n",
    "        self.input_shape = model_inputs[0].shape\n",
    "        self.input_height = self.input_shape[2]\n",
    "        self.input_width = self.input_shape[3]\n",
    "\n",
    "    def get_output_details(self):\n",
    "        model_outputs = self.session.get_outputs()\n",
    "        self.output_names = [model_outputs[i].name for i in range(len(model_outputs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e489ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imread(filename, flags=cv2.IMREAD_COLOR, dtype=np.uint8):\n",
    "    try:\n",
    "        n = np.fromfile(filename, dtype)\n",
    "        img = cv2.imdecode(n, flags)\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29eddbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\preparation\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Ultralytics YOLOv8.0.203  Python-3.9.16 torch-1.12.1 CPU (13th Gen Intel Core(TM) i9-13900K)\n",
      "Model summary (fused): 218 layers, 25843234 parameters, 0 gradients, 78.7 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'C:\\Users\\USER\\runs\\detect\\train77\\weights\\best.pt' with input shape (1, 3, 256, 256) BCHW and output shape(s) (1, 10, 1344) (49.6 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 10...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  1.8s, saved as 'C:\\Users\\USER\\runs\\detect\\train77\\weights\\best.onnx' (98.6 MB)\n",
      "\n",
      "Export complete (3.7s)\n",
      "Results saved to \u001b[1mC:\\Users\\USER\\runs\\detect\\train77\\weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=C:\\Users\\USER\\runs\\detect\\train77\\weights\\best.onnx imgsz=256  \n",
      "Validate:        yolo val task=detect model=C:\\Users\\USER\\runs\\detect\\train77\\weights\\best.onnx imgsz=256 data=C:\\Users\\USER\\Desktop\\\\  6\\ \\code\\image code\\data2.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"C:/Users/USER/runs/detect/train77/weights/best.pt\")\n",
    "model.export(format=\"onnx\", imgsz=[256,256])\n",
    "model_path = \"C:/Users/USER/runs/detect/train77/weights/best.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2f75ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from yolov8 import YOLOv8\n",
    "model_path = \"C:/Users/USER/runs/detect/train77/weights/best.onnx\"\n",
    "yolov8_detector = YOLOv8(model_path, conf_thres=0.5, iou_thres=0.3)\n",
    "path = os.getcwd()\n",
    "img = imread(os.path.join(os.path.join(os.path.join(path,\"img_data\"),\"All_img\"),\"00003.jpg\"))\n",
    "boxes, scores, class_ids = yolov8_detector(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ba39704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 5] [    0.73493     0.72919     0.82198]\n"
     ]
    }
   ],
   "source": [
    "print(class_ids,scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b25c73c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m combined_img \u001b[38;5;241m=\u001b[39m \u001b[43myolov8_detector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_detections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m cv2\u001b[38;5;241m.\u001b[39mnamedWindow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected Objects\u001b[39m\u001b[38;5;124m\"\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mWINDOW_NORMAL)\n\u001b[0;32m      3\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected Objects\u001b[39m\u001b[38;5;124m\"\u001b[39m, combined_img)\n",
      "Cell \u001b[1;32mIn[2], line 98\u001b[0m, in \u001b[0;36mYOLOv8.draw_detections\u001b[1;34m(self, image, draw_scores, mask_alpha)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_detections\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, draw_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mask_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m):\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_detections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_alpha\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\수\\인공지능 데브코스 6기\\최종 프로젝트\\code\\image code\\yolov8\\utils.py:91\u001b[0m, in \u001b[0;36mdraw_detections\u001b[1;34m(image, boxes, scores, class_ids, mask_alpha)\u001b[0m\n\u001b[0;32m     88\u001b[0m font_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m([img_height, img_width]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.0006\u001b[39m\n\u001b[0;32m     89\u001b[0m text_thickness \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmin\u001b[39m([img_height, img_width]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.000001\u001b[39m)\n\u001b[1;32m---> 91\u001b[0m det_img \u001b[38;5;241m=\u001b[39m \u001b[43mdraw_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdet_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_alpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Draw bounding boxes and labels of detections\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_id, box, score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(class_ids, boxes, scores):\n",
      "File \u001b[1;32m~\\Desktop\\수\\인공지능 데브코스 6기\\최종 프로젝트\\code\\image code\\yolov8\\utils.py:127\u001b[0m, in \u001b[0;36mdraw_masks\u001b[1;34m(image, boxes, classes, mask_alpha)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Draw bounding boxes and labels of detections\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m box, class_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(boxes, classes):\n\u001b[1;32m--> 127\u001b[0m     color \u001b[38;5;241m=\u001b[39m \u001b[43mcolors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclass_id\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    129\u001b[0m     x1, y1, x2, y2 \u001b[38;5;241m=\u001b[39m box\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# Draw fill rectangle in mask image\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 5 is out of bounds for axis 0 with size 5"
     ]
    }
   ],
   "source": [
    "combined_img = yolov8_detector.draw_detections(img)\n",
    "cv2.namedWindow(\"Detected Objects\", cv2.WINDOW_NORMAL)\n",
    "cv2.imshow(\"Detected Objects\", combined_img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484d01f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
